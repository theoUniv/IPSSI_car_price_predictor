# üöó **Car Price Predictor** üöó

Ce projet a pour but de cr√©er un pipeline complet pour la pr√©diction du prix des voitures d'occasion. Il se compose de trois √©tapes principales :

Collecte de Donn√©es (Scraping) : Utilisation de Scrapy et Playwright pour extraire les fiches techniques des v√©hicules depuis le site Autosphere.

Pr√©paration des Donn√©es (ETL) : Nettoyage, normalisation, et ing√©nierie de variables (comme le calcul de l'√¢ge du v√©hicule) pour structurer les donn√©es dans un fichier dataset.csv.

Mod√©lisation (Machine Learning) : Entra√Ænement d'un mod√®le de r√©gression XGBoost pour pr√©dire le prix des voitures en fonction de leurs caract√©ristiques.

Pr√©requis

Assurez-vous d'avoir Python 3.8+ et les outils suivants install√©s.

Installation des d√©pendances Python

Ce projet n√©cessite plusieurs biblioth√®ques, notamment pour le scraping (Scrapy, Playwright) et le Machine Learning (Pandas, Scikit-learn, XGBoost).

pour les installer : 
` pip install scrapy scrapy-playwright pandas scikit-learn xgboost `

# Installez les navigateurs n√©cessaires pour Playwright
` playwright install `


üõ†Ô∏è Utilisation

Le pipeline complet se d√©roule en trois √©tapes :

√âtape 1 : Collecte des Donn√©es (Scraping)

Le spider spider_corrig√©.py parcourt les pages de recherche et collecte les donn√©es de chaque fiche v√©hicule en utilisant Playwright pour g√©rer les chargements JavaScript.

Lancer le Spider :
Ex√©cutez cette commande √† la racine de votre projet. Les fichiers JSON seront sauvegard√©s dans le dossier scrapped/.

scrapy runspider spider_corrig√©.py


Note: Le spider est configur√© pour collecter un maximum de 100 pages par d√©faut (MAX_PAGES). Vous pouvez ajuster cette valeur dans le fichier spider_corrig√©.py si n√©cessaire.

√âtape 2 : Pr√©paration du Dataset (JSON vers CSV)

Une fois les donn√©es brutes collect√©es en JSON, le script de traitement nettoie les valeurs, calcule des variables importantes (comme l'√¢ge en ann√©es) et consolide tout en un fichier dataset.csv pr√™t pour l'entra√Ænement.

Ex√©cuter le script ETL :
Ce script est bas√© sur la logique vue dans votre fichier JsonToCsv.py.

python3 JsonToCsv.py
# Ceci va lire les fichiers dans 'scrapped/' et g√©n√©rer 'dataset.csv'


√âtape 3 : Entra√Ænement du Mod√®le et Pr√©diction

Le script d'entra√Ænement utilise le dataset.csv pour former le mod√®le XGBoost, √©value sa performance (RMSE), et effectue une pr√©diction sur un exemple de configuration de voiture.

Cr√©er un fichier de configuration pour la pr√©diction :
Cr√©ez un dossier to_predict/ et un fichier car_config.json √† la racine pour tester votre mod√®le.

Exemple de to_predict/car_config.json:

{
    "marque": "PEUGEOT",
    "modele": "208",
    "energie": "essence",
    "boite_de_vitesses": "manuelle",
    "couleur": "gris",
    "type_vehicule": "citadine",
    "provenance": "FRANCE",
    "premiere_main": "Non",
    "kilometrage": 50000,
    "puissance_fiscale": 5,
    "puissance_reelle": 100,
    "portes": 5,
    "places": 5,
    "age_ans": 3
}


Lancer l'entra√Ænement et la pr√©diction :
Ce script est bas√© sur la logique vue dans votre fichier train_model.py.

python3 train_model.py
# Le script affichera le RMSE et le prix pr√©dit pour la voiture dans car_config.json.


Structure du Projet

.
‚îú‚îÄ‚îÄ scrapped/                 # Dossier o√π les fichiers JSON bruts sont stock√©s
‚îÇ   ‚îú‚îÄ‚îÄ fiche_0.json
‚îÇ   ‚îú‚îÄ‚îÄ fiche_1.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ to_predict/               # Dossier pour la configuration de pr√©diction
‚îÇ   ‚îî‚îÄ‚îÄ car_config.json       # Fichier pour tester la pr√©diction finale
‚îú‚îÄ‚îÄ dataset.csv               # Dataset nettoy√©, pr√™t pour l'entra√Ænement (produit par JsonToCsv.py)
‚îú‚îÄ‚îÄ spider_corrig√©.py         # Spider Scrapy/Playwright (Collecte)
‚îú‚îÄ‚îÄ JsonToCsv.py              # Script de nettoyage et d'agr√©gation (ETL)
‚îî‚îÄ‚îÄ train_model.py            # Script d'entra√Ænement XGBoost (Mod√©lisation)
