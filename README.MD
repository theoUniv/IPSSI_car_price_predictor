# ğŸš— **Car Price Predictor** ğŸš—

Ce projet a pour but de crÃ©er un pipeline complet pour la prÃ©diction du prix des voitures d'occasion. Il se compose de trois Ã©tapes principales :

Collecte de DonnÃ©es (Scraping) : Utilisation de Scrapy et Playwright pour extraire les fiches techniques des vÃ©hicules depuis le site Autosphere.

PrÃ©paration des DonnÃ©es (ETL) : Nettoyage, normalisation, et ingÃ©nierie de variables (comme le calcul de l'Ã¢ge du vÃ©hicule) pour structurer les donnÃ©es dans un fichier dataset.csv.

ModÃ©lisation (Machine Learning) : EntraÃ®nement d'un modÃ¨le de rÃ©gression XGBoost pour prÃ©dire le prix des voitures en fonction de leurs caractÃ©ristiques.

PrÃ©requis

Assurez-vous d'avoir Python 3.8+ et les outils suivants installÃ©s.

Installation des dÃ©pendances Python

Ce projet nÃ©cessite plusieurs bibliothÃ¨ques, notamment pour le scraping (Scrapy, Playwright) et le Machine Learning (Pandas, Scikit-learn, XGBoost).

pour les installer : 
` pip install scrapy scrapy-playwright pandas scikit-learn xgboost `

# Installez les navigateurs nÃ©cessaires pour Playwright
` playwright install `


ğŸ› ï¸ Utilisation

Le pipeline complet se dÃ©roule en trois Ã©tapes :

Ã‰tape 1 : Collecte des DonnÃ©es (Scraping)

Le spider spider_corrigÃ©.py parcourt les pages de recherche et collecte les donnÃ©es de chaque fiche vÃ©hicule en utilisant Playwright pour gÃ©rer les chargements JavaScript.

Lancer le Spider :
ExÃ©cutez cette commande Ã  la racine de votre projet. Les fichiers JSON seront sauvegardÃ©s dans le dossier scrapped/.

` scrapy runspider spider_corrigÃ©.py `


Note: Le spider est configurÃ© pour collecter un maximum de 100 pages par dÃ©faut (MAX_PAGES). Vous pouvez ajuster cette valeur dans le fichier spider_corrigÃ©.py si nÃ©cessaire.

Ã‰tape 2 : PrÃ©paration du Dataset (JSON vers CSV)

Une fois les donnÃ©es brutes collectÃ©es en JSON, le script de traitement nettoie les valeurs, calcule des variables importantes (comme l'Ã¢ge en annÃ©es) et consolide tout en un fichier dataset.csv prÃªt pour l'entraÃ®nement.

ExÃ©cuter le script ETL :

` python3 JsonToCsv.py `
# Ceci va lire les fichiers dans 'scrapped/' et gÃ©nÃ©rer 'dataset.csv'


Ã‰tape 3 : EntraÃ®nement du ModÃ¨le et PrÃ©diction

Le script d'entraÃ®nement utilise le dataset.csv pour former le modÃ¨le XGBoost, Ã©value sa performance (RMSE), et effectue une prÃ©diction sur un exemple de configuration de voiture.

CrÃ©er un fichier de configuration pour la prÃ©diction :
CrÃ©ez un dossier to_predict/ et un fichier car_config.json Ã  la racine pour tester votre modÃ¨le.

Exemple de to_predict/car_config.json:

{
    "marque": "PEUGEOT",
    "modele": "208",
    "energie": "essence",
    "boite_de_vitesses": "manuelle",
    "couleur": "gris",
    "type_vehicule": "citadine",
    "provenance": "FRANCE",
    "premiere_main": "Non",
    "kilometrage": 50000,
    "puissance_fiscale": 5,
    "puissance_reelle": 100,
    "portes": 5,
    "places": 5,
    "age_ans": 3
}


Lancer l'entraÃ®nement et la prÃ©diction :
Ce script est basÃ© sur la logique vue dans votre fichier train_model.py.

python3 train_model.py
# Le script affichera le RMSE et le prix prÃ©dit pour la voiture dans car_config.json.


Structure du Projet

.
â”œâ”€â”€ scrapped/                 # Dossier oÃ¹ les fichiers JSON bruts sont stockÃ©s
â”‚   â”œâ”€â”€ fiche_0.json
â”‚   â”œâ”€â”€ fiche_1.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ to_predict/               # Dossier pour la configuration de prÃ©diction
â”‚   â””â”€â”€ car_config.json       # Fichier pour tester la prÃ©diction finale
â”œâ”€â”€ dataset.csv               # Dataset nettoyÃ©, prÃªt pour l'entraÃ®nement (produit par JsonToCsv.py)
â”œâ”€â”€ spider_corrigÃ©.py         # Spider Scrapy/Playwright (Collecte)
â”œâ”€â”€ JsonToCsv.py              # Script de nettoyage et d'agrÃ©gation (ETL)
â””â”€â”€ train_model.py            # Script d'entraÃ®nement XGBoost (ModÃ©lisation)
